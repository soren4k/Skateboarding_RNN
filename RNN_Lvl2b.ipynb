{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f392f4f-45f3-45a1-8f9c-2f068d61faf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "# Section 1: Imports and Device Setup\n",
    "###############################################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81552e7e-0a89-4135-9ea7-1e490ff1e7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 305\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "# Section 2: Data Loading and Preprocessing\n",
    "###############################################\n",
    "with open(\"combined_cleaned.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "data_encoded = np.array([stoi[ch] for ch in data], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4524e5f2-86dd-4339-aa61-479cab7f5c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Section 3: Model Definition and Global Functions\n",
    "###############################################\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers=1, dropout=0.0):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        # GRU applies dropout between layers if num_layers > 1\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embed(x)  # (batch, seq_len, hidden_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "        out = self.fc(out)  # (batch*seq_len, vocab_size)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "def get_batch(data, batch_size, seq_length):\n",
    "    indices = np.random.randint(0, len(data) - seq_length - 1, batch_size)\n",
    "    x_batch = np.array([data[i:i+seq_length] for i in indices])\n",
    "    y_batch = np.array([data[i+1:i+seq_length+1] for i in indices])\n",
    "    return (torch.tensor(x_batch, dtype=torch.long).to(device),\n",
    "            torch.tensor(y_batch, dtype=torch.long).to(device))\n",
    "\n",
    "def generate_text(model, start_text=\"the \", length=300, temperature=1.0):\n",
    "    start_text = start_text.lower()\n",
    "    chars_generated = list(start_text)\n",
    "    hidden = model.init_hidden(1)\n",
    "    input_seq = torch.tensor([[stoi[ch] for ch in start_text if ch in stoi]], dtype=torch.long).to(device)\n",
    "    _, hidden = model(input_seq, hidden)\n",
    "    last_char = input_seq[0, -1]\n",
    "    for _ in range(length):\n",
    "        output, hidden = model(last_char.view(1, 1), hidden)\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        char = itos[top_i.item()]\n",
    "        chars_generated.append(char)\n",
    "        last_char = torch.tensor([stoi[char]], dtype=torch.long).to(device)\n",
    "    return \"\".join(chars_generated)\n",
    "\n",
    "def log_experiment(trial, config, train_loss, val_loss, csv_path='lvl2b_log.csv'):\n",
    "    data = {\n",
    "        'experiment_number': trial,\n",
    "        'learning_rate': config['lr'],\n",
    "        'batch_size': config['batch_size'],\n",
    "        'seq_length': config['seq_length'],\n",
    "        'hidden_size': config['hidden_size'],\n",
    "        'dropout': config['dropout'],\n",
    "        'num_epochs': config['num_epochs'],\n",
    "        'training_loss': train_loss,\n",
    "        'validation_loss': val_loss,\n",
    "        'training_perplexity': np.exp(train_loss),\n",
    "        'validation_perplexity': np.exp(val_loss)\n",
    "    }\n",
    "    df_new = pd.DataFrame([data])\n",
    "    try:\n",
    "        df_existing = pd.read_csv(csv_path)\n",
    "        df = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "    except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "        df = df_new\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ccd5d98-d21c-441a-8d53-7999a88ae3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trials to run: 16\n",
      "\n",
      "=== Starting Trial 1 with config: {'lr': 0.003, 'batch_size': 128, 'seq_length': 100, 'hidden_size': 512, 'dropout': 0.0, 'num_layers': 1, 'num_epochs': 10} ===\n",
      "Trial 1, Epoch [1/10], Batch [100/347], Loss: 1.7342\n",
      "Trial 1, Epoch [1/10], Batch [200/347], Loss: 1.5762\n",
      "Trial 1, Epoch [1/10], Batch [300/347], Loss: 1.4992\n",
      "Trial 1, Epoch [1/10] Average Training Loss: 1.7106\n",
      "Trial 1, Epoch [1/10] Average Validation Loss: 1.4689\n",
      "Trial 1, Epoch [2/10], Batch [100/347], Loss: 1.4645\n",
      "Trial 1, Epoch [2/10], Batch [200/347], Loss: 1.4621\n",
      "Trial 1, Epoch [2/10], Batch [300/347], Loss: 1.3954\n",
      "Trial 1, Epoch [2/10] Average Training Loss: 1.4393\n",
      "Trial 1, Epoch [2/10] Average Validation Loss: 1.3921\n",
      "Trial 1, Epoch [3/10], Batch [100/347], Loss: 1.3526\n",
      "Trial 1, Epoch [3/10], Batch [200/347], Loss: 1.3731\n",
      "Trial 1, Epoch [3/10], Batch [300/347], Loss: 1.3728\n",
      "Trial 1, Epoch [3/10] Average Training Loss: 1.3852\n",
      "Trial 1, Epoch [3/10] Average Validation Loss: 1.3650\n",
      "Trial 1, Epoch [4/10], Batch [100/347], Loss: 1.3915\n",
      "Trial 1, Epoch [4/10], Batch [200/347], Loss: 1.3717\n",
      "Trial 1, Epoch [4/10], Batch [300/347], Loss: 1.3307\n",
      "Trial 1, Epoch [4/10] Average Training Loss: 1.3554\n",
      "Trial 1, Epoch [4/10] Average Validation Loss: 1.3302\n",
      "Trial 1, Epoch [5/10], Batch [100/347], Loss: 1.3474\n",
      "Trial 1, Epoch [5/10], Batch [200/347], Loss: 1.3317\n",
      "Trial 1, Epoch [5/10], Batch [300/347], Loss: 1.3234\n",
      "Trial 1, Epoch [5/10] Average Training Loss: 1.3348\n",
      "Trial 1, Epoch [5/10] Average Validation Loss: 1.3275\n",
      "Trial 1, Epoch [6/10], Batch [100/347], Loss: 1.3143\n",
      "Trial 1, Epoch [6/10], Batch [200/347], Loss: 1.3100\n",
      "Trial 1, Epoch [6/10], Batch [300/347], Loss: 1.3017\n",
      "Trial 1, Epoch [6/10] Average Training Loss: 1.3197\n",
      "Trial 1, Epoch [6/10] Average Validation Loss: 1.3197\n",
      "Trial 1, Epoch [7/10], Batch [100/347], Loss: 1.3116\n",
      "Trial 1, Epoch [7/10], Batch [200/347], Loss: 1.3444\n",
      "Trial 1, Epoch [7/10], Batch [300/347], Loss: 1.3040\n",
      "Trial 1, Epoch [7/10] Average Training Loss: 1.3079\n",
      "Trial 1, Epoch [7/10] Average Validation Loss: 1.3009\n",
      "Trial 1, Epoch [8/10], Batch [100/347], Loss: 1.2961\n",
      "Trial 1, Epoch [8/10], Batch [200/347], Loss: 1.2698\n",
      "Trial 1, Epoch [8/10], Batch [300/347], Loss: 1.2945\n",
      "Trial 1, Epoch [8/10] Average Training Loss: 1.2980\n",
      "Trial 1, Epoch [8/10] Average Validation Loss: 1.3139\n",
      "Trial 1, Epoch [9/10], Batch [100/347], Loss: 1.2381\n",
      "Trial 1, Epoch [9/10], Batch [200/347], Loss: 1.3260\n",
      "Trial 1, Epoch [9/10], Batch [300/347], Loss: 1.3068\n",
      "Trial 1, Epoch [9/10] Average Training Loss: 1.2926\n",
      "Trial 1, Epoch [9/10] Average Validation Loss: 1.3016\n",
      "Trial 1, Epoch [10/10], Batch [100/347], Loss: 1.2905\n",
      "Trial 1, Epoch [10/10], Batch [200/347], Loss: 1.3276\n",
      "Trial 1, Epoch [10/10], Batch [300/347], Loss: 1.2686\n",
      "Trial 1, Epoch [10/10] Average Training Loss: 1.2876\n",
      "Trial 1, Epoch [10/10] Average Validation Loss: 1.2956\n",
      "\n",
      "Trial 1 Sample Text:\n",
      "the brutand that got my mentioned about that i want to skate to this. but also well first it healing company. it was self-blouddent. that was trumie feeling skate, gus, the necession in the new york, and i rebought it and all this is to guate woring heroes cheap on the clips and i don’t need twith surprises, teamminatemaible said, i like everything these plazate for the meaning. it was me that enough in the morning. i would shoot what people try to say situtic tunnel dearlin, what i’ll really one le\n",
      "\n",
      "=== Starting Trial 2 with config: {'lr': 0.003, 'batch_size': 128, 'seq_length': 100, 'hidden_size': 256, 'dropout': 0.0, 'num_layers': 1, 'num_epochs': 10} ===\n",
      "Trial 2, Epoch [1/10], Batch [100/347], Loss: 1.8788\n",
      "Trial 2, Epoch [1/10], Batch [200/347], Loss: 1.7394\n",
      "Trial 2, Epoch [1/10], Batch [300/347], Loss: 1.5889\n",
      "Trial 2, Epoch [1/10] Average Training Loss: 1.8619\n",
      "Trial 2, Epoch [1/10] Average Validation Loss: 1.5407\n",
      "Trial 2, Epoch [2/10], Batch [100/347], Loss: 1.5632\n",
      "Trial 2, Epoch [2/10], Batch [200/347], Loss: 1.5168\n",
      "Trial 2, Epoch [2/10], Batch [300/347], Loss: 1.5123\n",
      "Trial 2, Epoch [2/10] Average Training Loss: 1.5330\n",
      "Trial 2, Epoch [2/10] Average Validation Loss: 1.4693\n",
      "Trial 2, Epoch [3/10], Batch [100/347], Loss: 1.5211\n",
      "Trial 2, Epoch [3/10], Batch [200/347], Loss: 1.4711\n",
      "Trial 2, Epoch [3/10], Batch [300/347], Loss: 1.4348\n",
      "Trial 2, Epoch [3/10] Average Training Loss: 1.4673\n",
      "Trial 2, Epoch [3/10] Average Validation Loss: 1.4326\n",
      "Trial 2, Epoch [4/10], Batch [100/347], Loss: 1.4334\n",
      "Trial 2, Epoch [4/10], Batch [200/347], Loss: 1.4125\n",
      "Trial 2, Epoch [4/10], Batch [300/347], Loss: 1.4064\n",
      "Trial 2, Epoch [4/10] Average Training Loss: 1.4322\n",
      "Trial 2, Epoch [4/10] Average Validation Loss: 1.4084\n",
      "Trial 2, Epoch [5/10], Batch [100/347], Loss: 1.4496\n",
      "Trial 2, Epoch [5/10], Batch [200/347], Loss: 1.4075\n",
      "Trial 2, Epoch [5/10], Batch [300/347], Loss: 1.3745\n",
      "Trial 2, Epoch [5/10] Average Training Loss: 1.4129\n",
      "Trial 2, Epoch [5/10] Average Validation Loss: 1.3953\n",
      "Trial 2, Epoch [6/10], Batch [100/347], Loss: 1.4233\n",
      "Trial 2, Epoch [6/10], Batch [200/347], Loss: 1.4114\n",
      "Trial 2, Epoch [6/10], Batch [300/347], Loss: 1.3822\n",
      "Trial 2, Epoch [6/10] Average Training Loss: 1.3925\n",
      "Trial 2, Epoch [6/10] Average Validation Loss: 1.3810\n",
      "Trial 2, Epoch [7/10], Batch [100/347], Loss: 1.3868\n",
      "Trial 2, Epoch [7/10], Batch [200/347], Loss: 1.4341\n",
      "Trial 2, Epoch [7/10], Batch [300/347], Loss: 1.3845\n",
      "Trial 2, Epoch [7/10] Average Training Loss: 1.3820\n",
      "Trial 2, Epoch [7/10] Average Validation Loss: 1.3674\n",
      "Trial 2, Epoch [8/10], Batch [100/347], Loss: 1.3574\n",
      "Trial 2, Epoch [8/10], Batch [200/347], Loss: 1.3642\n",
      "Trial 2, Epoch [8/10], Batch [300/347], Loss: 1.3581\n",
      "Trial 2, Epoch [8/10] Average Training Loss: 1.3704\n",
      "Trial 2, Epoch [8/10] Average Validation Loss: 1.3654\n",
      "Trial 2, Epoch [9/10], Batch [100/347], Loss: 1.3722\n",
      "Trial 2, Epoch [9/10], Batch [200/347], Loss: 1.3405\n",
      "Trial 2, Epoch [9/10], Batch [300/347], Loss: 1.3599\n",
      "Trial 2, Epoch [9/10] Average Training Loss: 1.3625\n",
      "Trial 2, Epoch [9/10] Average Validation Loss: 1.3453\n",
      "Trial 2, Epoch [10/10], Batch [100/347], Loss: 1.3845\n",
      "Trial 2, Epoch [10/10], Batch [200/347], Loss: 1.3330\n",
      "Trial 2, Epoch [10/10], Batch [300/347], Loss: 1.3297\n",
      "Trial 2, Epoch [10/10] Average Training Loss: 1.3570\n",
      "Trial 2, Epoch [10/10] Average Validation Loss: 1.3554\n",
      "\n",
      "Trial 2 Sample Text:\n",
      "the wanted to a!\n",
      "you like the pros gall around that it was the rey incing by trying to film of an ademian 50-50ing literage called prettys contests and happy depend and publical kind of shit play world\n",
      "mike.\n",
      "you called few if i called good “so that’s up you?\n",
      "he beards boards at the group, i wasn’t the pressure you get seve all my of those of that soon frince. there was kind of furkin’b for me been on their way extremely just greats but i’ve had all i was the only deam son’s down intibilities.\n",
      "and yo\n",
      "\n",
      "=== Starting Trial 3 with config: {'lr': 0.003, 'batch_size': 128, 'seq_length': 150, 'hidden_size': 512, 'dropout': 0.0, 'num_layers': 1, 'num_epochs': 10} ===\n",
      "Trial 3, Epoch [1/10], Batch [100/231], Loss: 1.6867\n",
      "Trial 3, Epoch [1/10], Batch [200/231], Loss: 1.5290\n",
      "Trial 3, Epoch [1/10] Average Training Loss: 1.7858\n",
      "Trial 3, Epoch [1/10] Average Validation Loss: 1.4811\n",
      "Trial 3, Epoch [2/10], Batch [100/231], Loss: 1.4608\n",
      "Trial 3, Epoch [2/10], Batch [200/231], Loss: 1.4309\n",
      "Trial 3, Epoch [2/10] Average Training Loss: 1.4449\n",
      "Trial 3, Epoch [2/10] Average Validation Loss: 1.3813\n",
      "Trial 3, Epoch [3/10], Batch [100/231], Loss: 1.3496\n",
      "Trial 3, Epoch [3/10], Batch [200/231], Loss: 1.3512\n",
      "Trial 3, Epoch [3/10] Average Training Loss: 1.3738\n",
      "Trial 3, Epoch [3/10] Average Validation Loss: 1.3455\n",
      "Trial 3, Epoch [4/10], Batch [100/231], Loss: 1.3505\n",
      "Trial 3, Epoch [4/10], Batch [200/231], Loss: 1.3779\n",
      "Trial 3, Epoch [4/10] Average Training Loss: 1.3392\n",
      "Trial 3, Epoch [4/10] Average Validation Loss: 1.3210\n",
      "Trial 3, Epoch [5/10], Batch [100/231], Loss: 1.3175\n",
      "Trial 3, Epoch [5/10], Batch [200/231], Loss: 1.3308\n",
      "Trial 3, Epoch [5/10] Average Training Loss: 1.3166\n",
      "Trial 3, Epoch [5/10] Average Validation Loss: 1.3124\n",
      "Trial 3, Epoch [6/10], Batch [100/231], Loss: 1.2783\n",
      "Trial 3, Epoch [6/10], Batch [200/231], Loss: 1.2937\n",
      "Trial 3, Epoch [6/10] Average Training Loss: 1.3006\n",
      "Trial 3, Epoch [6/10] Average Validation Loss: 1.3089\n",
      "Trial 3, Epoch [7/10], Batch [100/231], Loss: 1.3052\n",
      "Trial 3, Epoch [7/10], Batch [200/231], Loss: 1.3120\n",
      "Trial 3, Epoch [7/10] Average Training Loss: 1.2861\n",
      "Trial 3, Epoch [7/10] Average Validation Loss: 1.2921\n",
      "Trial 3, Epoch [8/10], Batch [100/231], Loss: 1.2185\n",
      "Trial 3, Epoch [8/10], Batch [200/231], Loss: 1.2837\n",
      "Trial 3, Epoch [8/10] Average Training Loss: 1.2754\n",
      "Trial 3, Epoch [8/10] Average Validation Loss: 1.2758\n",
      "Trial 3, Epoch [9/10], Batch [100/231], Loss: 1.2790\n",
      "Trial 3, Epoch [9/10], Batch [200/231], Loss: 1.2500\n",
      "Trial 3, Epoch [9/10] Average Training Loss: 1.2653\n",
      "Trial 3, Epoch [9/10] Average Validation Loss: 1.2845\n",
      "Trial 3, Epoch [10/10], Batch [100/231], Loss: 1.2366\n",
      "Trial 3, Epoch [10/10], Batch [200/231], Loss: 1.2428\n",
      "Trial 3, Epoch [10/10] Average Training Loss: 1.2618\n",
      "Trial 3, Epoch [10/10] Average Validation Loss: 1.2652\n",
      "\n",
      "Trial 3 Sample Text:\n",
      "the exactly very scene in a 50-50 for a flattby and young job back to, i just don’t know how it can watch skate call in 2024.\n",
      "what dictor’s artcent cover from the sile implied there’s good.\n",
      "have you continue it out when he hood and funds the varial has who it seeing missed together together like a seckurding for so once your skating last milson again. with privated style. there are going to gelp, he's to build full-fool-to work that he’s soon, and creation of sparkets with streez, vanzeform, so many\n",
      "\n",
      "=== Starting Trial 4 with config: {'lr': 0.003, 'batch_size': 128, 'seq_length': 150, 'hidden_size': 256, 'dropout': 0.0, 'num_layers': 1, 'num_epochs': 10} ===\n",
      "Trial 4, Epoch [1/10], Batch [100/231], Loss: 1.8815\n",
      "Trial 4, Epoch [1/10], Batch [200/231], Loss: 1.6700\n",
      "Trial 4, Epoch [1/10] Average Training Loss: 1.9779\n",
      "Trial 4, Epoch [1/10] Average Validation Loss: 1.6081\n",
      "Trial 4, Epoch [2/10], Batch [100/231], Loss: 1.6107\n",
      "Trial 4, Epoch [2/10], Batch [200/231], Loss: 1.5238\n",
      "Trial 4, Epoch [2/10] Average Training Loss: 1.5728\n",
      "Trial 4, Epoch [2/10] Average Validation Loss: 1.4813\n",
      "Trial 4, Epoch [3/10], Batch [100/231], Loss: 1.5164\n",
      "Trial 4, Epoch [3/10], Batch [200/231], Loss: 1.4850\n",
      "Trial 4, Epoch [3/10] Average Training Loss: 1.4922\n",
      "Trial 4, Epoch [3/10] Average Validation Loss: 1.4429\n",
      "Trial 4, Epoch [4/10], Batch [100/231], Loss: 1.4900\n",
      "Trial 4, Epoch [4/10], Batch [200/231], Loss: 1.4643\n",
      "Trial 4, Epoch [4/10] Average Training Loss: 1.4571\n",
      "Trial 4, Epoch [4/10] Average Validation Loss: 1.4273\n",
      "Trial 4, Epoch [5/10], Batch [100/231], Loss: 1.4385\n",
      "Trial 4, Epoch [5/10], Batch [200/231], Loss: 1.3838\n",
      "Trial 4, Epoch [5/10] Average Training Loss: 1.4257\n",
      "Trial 4, Epoch [5/10] Average Validation Loss: 1.4061\n",
      "Trial 4, Epoch [6/10], Batch [100/231], Loss: 1.3450\n",
      "Trial 4, Epoch [6/10], Batch [200/231], Loss: 1.3918\n",
      "Trial 4, Epoch [6/10] Average Training Loss: 1.4096\n",
      "Trial 4, Epoch [6/10] Average Validation Loss: 1.4033\n",
      "Trial 4, Epoch [7/10], Batch [100/231], Loss: 1.3844\n",
      "Trial 4, Epoch [7/10], Batch [200/231], Loss: 1.3732\n",
      "Trial 4, Epoch [7/10] Average Training Loss: 1.3916\n",
      "Trial 4, Epoch [7/10] Average Validation Loss: 1.3790\n",
      "Trial 4, Epoch [8/10], Batch [100/231], Loss: 1.3686\n",
      "Trial 4, Epoch [8/10], Batch [200/231], Loss: 1.3906\n",
      "Trial 4, Epoch [8/10] Average Training Loss: 1.3789\n",
      "Trial 4, Epoch [8/10] Average Validation Loss: 1.3598\n",
      "Trial 4, Epoch [9/10], Batch [100/231], Loss: 1.3692\n",
      "Trial 4, Epoch [9/10], Batch [200/231], Loss: 1.3829\n",
      "Trial 4, Epoch [9/10] Average Training Loss: 1.3692\n",
      "Trial 4, Epoch [9/10] Average Validation Loss: 1.3547\n",
      "Trial 4, Epoch [10/10], Batch [100/231], Loss: 1.3476\n",
      "Trial 4, Epoch [10/10], Batch [200/231], Loss: 1.3424\n",
      "Trial 4, Epoch [10/10] Average Training Loss: 1.3590\n",
      "Trial 4, Epoch [10/10] Average Validation Loss: 1.3580\n",
      "\n",
      "Trial 4 Sample Text:\n",
      "the was being me. it’s hard the name! i don’t shown there to make if a house hip at worked on soth heart london in really. alletting was going live there at those instantly filming fur bome. the trick edit, because they do smoot-year in your kyle. it’s misme it on jante free culture off they always do in designer in some stations prude. i first favivering as i know the something was not from too song…\n",
      "what wastent missis?\n",
      "i was sotbs to great that in the progcool. there’s been hard up nosesion pairs\n",
      "\n",
      "=== Starting Trial 5 with config: {'lr': 0.003, 'batch_size': 64, 'seq_length': 100, 'hidden_size': 512, 'dropout': 0.0, 'num_layers': 1, 'num_epochs': 10} ===\n",
      "Trial 5, Epoch [1/10], Batch [100/694], Loss: 1.7300\n",
      "Trial 5, Epoch [1/10], Batch [200/694], Loss: 1.7034\n",
      "Trial 5, Epoch [1/10], Batch [300/694], Loss: 1.6315\n",
      "Trial 5, Epoch [1/10], Batch [400/694], Loss: 1.5536\n",
      "Trial 5, Epoch [1/10], Batch [500/694], Loss: 1.4574\n",
      "Trial 5, Epoch [1/10], Batch [600/694], Loss: 1.4736\n",
      "Trial 5, Epoch [1/10] Average Training Loss: 1.6245\n",
      "Trial 5, Epoch [1/10] Average Validation Loss: 1.4480\n",
      "Trial 5, Epoch [2/10], Batch [100/694], Loss: 1.4068\n",
      "Trial 5, Epoch [2/10], Batch [200/694], Loss: 1.4561\n",
      "Trial 5, Epoch [2/10], Batch [300/694], Loss: 1.4093\n",
      "Trial 5, Epoch [2/10], Batch [400/694], Loss: 1.4749\n",
      "Trial 5, Epoch [2/10], Batch [500/694], Loss: 1.4210\n",
      "Trial 5, Epoch [2/10], Batch [600/694], Loss: 1.4198\n",
      "Trial 5, Epoch [2/10] Average Training Loss: 1.4304\n",
      "Trial 5, Epoch [2/10] Average Validation Loss: 1.3886\n",
      "Trial 5, Epoch [3/10], Batch [100/694], Loss: 1.3960\n",
      "Trial 5, Epoch [3/10], Batch [200/694], Loss: 1.3741\n",
      "Trial 5, Epoch [3/10], Batch [300/694], Loss: 1.4335\n",
      "Trial 5, Epoch [3/10], Batch [400/694], Loss: 1.3769\n",
      "Trial 5, Epoch [3/10], Batch [500/694], Loss: 1.3800\n",
      "Trial 5, Epoch [3/10], Batch [600/694], Loss: 1.3734\n",
      "Trial 5, Epoch [3/10] Average Training Loss: 1.3870\n",
      "Trial 5, Epoch [3/10] Average Validation Loss: 1.3653\n",
      "Trial 5, Epoch [4/10], Batch [100/694], Loss: 1.3986\n",
      "Trial 5, Epoch [4/10], Batch [200/694], Loss: 1.3478\n",
      "Trial 5, Epoch [4/10], Batch [300/694], Loss: 1.3904\n",
      "Trial 5, Epoch [4/10], Batch [400/694], Loss: 1.3631\n",
      "Trial 5, Epoch [4/10], Batch [500/694], Loss: 1.3620\n",
      "Trial 5, Epoch [4/10], Batch [600/694], Loss: 1.3701\n",
      "Trial 5, Epoch [4/10] Average Training Loss: 1.3627\n",
      "Trial 5, Epoch [4/10] Average Validation Loss: 1.3449\n",
      "Trial 5, Epoch [5/10], Batch [100/694], Loss: 1.3556\n",
      "Trial 5, Epoch [5/10], Batch [200/694], Loss: 1.3482\n",
      "Trial 5, Epoch [5/10], Batch [300/694], Loss: 1.3514\n",
      "Trial 5, Epoch [5/10], Batch [400/694], Loss: 1.3464\n",
      "Trial 5, Epoch [5/10], Batch [500/694], Loss: 1.3081\n",
      "Trial 5, Epoch [5/10], Batch [600/694], Loss: 1.2748\n",
      "Trial 5, Epoch [5/10] Average Training Loss: 1.3462\n",
      "Trial 5, Epoch [5/10] Average Validation Loss: 1.3388\n",
      "Trial 5, Epoch [6/10], Batch [100/694], Loss: 1.3307\n",
      "Trial 5, Epoch [6/10], Batch [200/694], Loss: 1.3335\n",
      "Trial 5, Epoch [6/10], Batch [300/694], Loss: 1.3912\n",
      "Trial 5, Epoch [6/10], Batch [400/694], Loss: 1.2935\n",
      "Trial 5, Epoch [6/10], Batch [500/694], Loss: 1.3508\n",
      "Trial 5, Epoch [6/10], Batch [600/694], Loss: 1.3428\n",
      "Trial 5, Epoch [6/10] Average Training Loss: 1.3361\n",
      "Trial 5, Epoch [6/10] Average Validation Loss: 1.3414\n",
      "Trial 5, Epoch [7/10], Batch [100/694], Loss: 1.3367\n",
      "Trial 5, Epoch [7/10], Batch [200/694], Loss: 1.3646\n",
      "Trial 5, Epoch [7/10], Batch [300/694], Loss: 1.3604\n",
      "Trial 5, Epoch [7/10], Batch [400/694], Loss: 1.3826\n",
      "Trial 5, Epoch [7/10], Batch [500/694], Loss: 1.3361\n",
      "Trial 5, Epoch [7/10], Batch [600/694], Loss: 1.2872\n",
      "Trial 5, Epoch [7/10] Average Training Loss: 1.3284\n",
      "Trial 5, Epoch [7/10] Average Validation Loss: 1.3282\n",
      "Trial 5, Epoch [8/10], Batch [100/694], Loss: 1.3080\n",
      "Trial 5, Epoch [8/10], Batch [200/694], Loss: 1.3279\n",
      "Trial 5, Epoch [8/10], Batch [300/694], Loss: 1.3527\n",
      "Trial 5, Epoch [8/10], Batch [400/694], Loss: 1.3278\n",
      "Trial 5, Epoch [8/10], Batch [500/694], Loss: 1.3427\n",
      "Trial 5, Epoch [8/10], Batch [600/694], Loss: 1.3145\n",
      "Trial 5, Epoch [8/10] Average Training Loss: 1.3193\n",
      "Trial 5, Epoch [8/10] Average Validation Loss: 1.3263\n",
      "Trial 5, Epoch [9/10], Batch [100/694], Loss: 1.2847\n",
      "Trial 5, Epoch [9/10], Batch [200/694], Loss: 1.3111\n",
      "Trial 5, Epoch [9/10], Batch [300/694], Loss: 1.3369\n",
      "Trial 5, Epoch [9/10], Batch [400/694], Loss: 1.3053\n",
      "Trial 5, Epoch [9/10], Batch [500/694], Loss: 1.3308\n",
      "Trial 5, Epoch [9/10], Batch [600/694], Loss: 1.3548\n",
      "Trial 5, Epoch [9/10] Average Training Loss: 1.3176\n",
      "Trial 5, Epoch [9/10] Average Validation Loss: 1.3288\n",
      "Trial 5, Epoch [10/10], Batch [100/694], Loss: 1.3193\n",
      "Trial 5, Epoch [10/10], Batch [200/694], Loss: 1.3649\n",
      "Trial 5, Epoch [10/10], Batch [300/694], Loss: 1.3266\n",
      "Trial 5, Epoch [10/10], Batch [400/694], Loss: 1.2952\n",
      "Trial 5, Epoch [10/10], Batch [500/694], Loss: 1.3032\n",
      "Trial 5, Epoch [10/10], Batch [600/694], Loss: 1.2765\n",
      "Trial 5, Epoch [10/10] Average Training Loss: 1.3157\n",
      "Trial 5, Epoch [10/10] Average Validation Loss: 1.3214\n",
      "\n",
      "Trial 5 Sample Text:\n",
      "the vinabio because you know, all like a hooks up with me all the quintle-towend sneekey. place where are some hyped to it are paris. designers who was the for about the messiled bar tends a night can oje about my homie about how you learned to interview, say the time, but all the work now linpossioned behavion funny love what private parks, and africa crimen, but where you from ideas up so it got us recently might wanted to not get than i take its part of the blueprint, and they’ve five-native. you\n",
      "\n",
      "=== Starting Trial 6 with config: {'lr': 0.003, 'batch_size': 64, 'seq_length': 100, 'hidden_size': 256, 'dropout': 0.0, 'num_layers': 1, 'num_epochs': 10} ===\n",
      "Trial 6, Epoch [1/10], Batch [100/694], Loss: 1.9531\n",
      "Trial 6, Epoch [1/10], Batch [200/694], Loss: 1.6872\n",
      "Trial 6, Epoch [1/10], Batch [300/694], Loss: 1.6216\n",
      "Trial 6, Epoch [1/10], Batch [400/694], Loss: 1.5429\n",
      "Trial 6, Epoch [1/10], Batch [500/694], Loss: 1.5390\n",
      "Trial 6, Epoch [1/10], Batch [600/694], Loss: 1.5467\n",
      "Trial 6, Epoch [1/10] Average Training Loss: 1.7252\n",
      "Trial 6, Epoch [1/10] Average Validation Loss: 1.5002\n",
      "Trial 6, Epoch [2/10], Batch [100/694], Loss: 1.5171\n",
      "Trial 6, Epoch [2/10], Batch [200/694], Loss: 1.5087\n",
      "Trial 6, Epoch [2/10], Batch [300/694], Loss: 1.5269\n",
      "Trial 6, Epoch [2/10], Batch [400/694], Loss: 1.4696\n",
      "Trial 6, Epoch [2/10], Batch [500/694], Loss: 1.5216\n",
      "Trial 6, Epoch [2/10], Batch [600/694], Loss: 1.4042\n",
      "Trial 6, Epoch [2/10] Average Training Loss: 1.4917\n",
      "Trial 6, Epoch [2/10] Average Validation Loss: 1.4350\n",
      "Trial 6, Epoch [3/10], Batch [100/694], Loss: 1.4585\n",
      "Trial 6, Epoch [3/10], Batch [200/694], Loss: 1.5143\n",
      "Trial 6, Epoch [3/10], Batch [300/694], Loss: 1.4558\n",
      "Trial 6, Epoch [3/10], Batch [400/694], Loss: 1.4371\n",
      "Trial 6, Epoch [3/10], Batch [500/694], Loss: 1.4378\n",
      "Trial 6, Epoch [3/10], Batch [600/694], Loss: 1.4294\n",
      "Trial 6, Epoch [3/10] Average Training Loss: 1.4401\n",
      "Trial 6, Epoch [3/10] Average Validation Loss: 1.4133\n",
      "Trial 6, Epoch [4/10], Batch [100/694], Loss: 1.4039\n",
      "Trial 6, Epoch [4/10], Batch [200/694], Loss: 1.4709\n",
      "Trial 6, Epoch [4/10], Batch [300/694], Loss: 1.4853\n",
      "Trial 6, Epoch [4/10], Batch [400/694], Loss: 1.4544\n",
      "Trial 6, Epoch [4/10], Batch [500/694], Loss: 1.4191\n",
      "Trial 6, Epoch [4/10], Batch [600/694], Loss: 1.4239\n",
      "Trial 6, Epoch [4/10] Average Training Loss: 1.4180\n",
      "Trial 6, Epoch [4/10] Average Validation Loss: 1.3958\n",
      "Trial 6, Epoch [5/10], Batch [100/694], Loss: 1.4282\n",
      "Trial 6, Epoch [5/10], Batch [200/694], Loss: 1.4162\n",
      "Trial 6, Epoch [5/10], Batch [300/694], Loss: 1.3840\n",
      "Trial 6, Epoch [5/10], Batch [400/694], Loss: 1.3667\n",
      "Trial 6, Epoch [5/10], Batch [500/694], Loss: 1.4622\n",
      "Trial 6, Epoch [5/10], Batch [600/694], Loss: 1.4167\n",
      "Trial 6, Epoch [5/10] Average Training Loss: 1.3966\n",
      "Trial 6, Epoch [5/10] Average Validation Loss: 1.3849\n",
      "Trial 6, Epoch [6/10], Batch [100/694], Loss: 1.3709\n",
      "Trial 6, Epoch [6/10], Batch [200/694], Loss: 1.3824\n",
      "Trial 6, Epoch [6/10], Batch [300/694], Loss: 1.3882\n",
      "Trial 6, Epoch [6/10], Batch [400/694], Loss: 1.3468\n",
      "Trial 6, Epoch [6/10], Batch [500/694], Loss: 1.3955\n",
      "Trial 6, Epoch [6/10], Batch [600/694], Loss: 1.3306\n",
      "Trial 6, Epoch [6/10] Average Training Loss: 1.3830\n",
      "Trial 6, Epoch [6/10] Average Validation Loss: 1.3730\n",
      "Trial 6, Epoch [7/10], Batch [100/694], Loss: 1.3834\n",
      "Trial 6, Epoch [7/10], Batch [200/694], Loss: 1.3967\n",
      "Trial 6, Epoch [7/10], Batch [300/694], Loss: 1.3615\n",
      "Trial 6, Epoch [7/10], Batch [400/694], Loss: 1.3088\n",
      "Trial 6, Epoch [7/10], Batch [500/694], Loss: 1.3861\n",
      "Trial 6, Epoch [7/10], Batch [600/694], Loss: 1.3342\n",
      "Trial 6, Epoch [7/10] Average Training Loss: 1.3708\n",
      "Trial 6, Epoch [7/10] Average Validation Loss: 1.3589\n",
      "Trial 6, Epoch [8/10], Batch [100/694], Loss: 1.3683\n",
      "Trial 6, Epoch [8/10], Batch [200/694], Loss: 1.3583\n",
      "Trial 6, Epoch [8/10], Batch [300/694], Loss: 1.3273\n",
      "Trial 6, Epoch [8/10], Batch [400/694], Loss: 1.3480\n",
      "Trial 6, Epoch [8/10], Batch [500/694], Loss: 1.3332\n",
      "Trial 6, Epoch [8/10], Batch [600/694], Loss: 1.3337\n",
      "Trial 6, Epoch [8/10] Average Training Loss: 1.3661\n",
      "Trial 6, Epoch [8/10] Average Validation Loss: 1.3568\n",
      "Trial 6, Epoch [9/10], Batch [100/694], Loss: 1.3896\n",
      "Trial 6, Epoch [9/10], Batch [200/694], Loss: 1.3179\n",
      "Trial 6, Epoch [9/10], Batch [300/694], Loss: 1.2786\n",
      "Trial 6, Epoch [9/10], Batch [400/694], Loss: 1.3905\n",
      "Trial 6, Epoch [9/10], Batch [500/694], Loss: 1.3927\n",
      "Trial 6, Epoch [9/10], Batch [600/694], Loss: 1.3602\n",
      "Trial 6, Epoch [9/10] Average Training Loss: 1.3598\n",
      "Trial 6, Epoch [9/10] Average Validation Loss: 1.3482\n",
      "Trial 6, Epoch [10/10], Batch [100/694], Loss: 1.3158\n",
      "Trial 6, Epoch [10/10], Batch [200/694], Loss: 1.3326\n",
      "Trial 6, Epoch [10/10], Batch [300/694], Loss: 1.3802\n",
      "Trial 6, Epoch [10/10], Batch [400/694], Loss: 1.3457\n",
      "Trial 6, Epoch [10/10], Batch [500/694], Loss: 1.3858\n",
      "Trial 6, Epoch [10/10], Batch [600/694], Loss: 1.2819\n",
      "Trial 6, Epoch [10/10] Average Training Loss: 1.3534\n",
      "Trial 6, Epoch [10/10] Average Validation Loss: 1.3710\n",
      "\n",
      "Trial 6 Sample Text:\n",
      "the of the res. so weren’t still save a last completely flishes.\n",
      "gabriel: y, 12 years is xelisn’t really got book the trip frontside from their way. still bring them but then i was just that it was going to get impodiling as i can’t because you can course in the bluno is. aatoric, if this is, you guy i do club sich picture “i first) versey. i’m doing any of get too.” with skating of have abootage in the thing. but people day do you have to bet the dowless of everything in no current story brotfli!\n",
      "w\n",
      "\n",
      "=== Starting Trial 7 with config: {'lr': 0.003, 'batch_size': 64, 'seq_length': 150, 'hidden_size': 512, 'dropout': 0.0, 'num_layers': 1, 'num_epochs': 10} ===\n",
      "Trial 7, Epoch [1/10], Batch [100/462], Loss: 1.7708\n",
      "Trial 7, Epoch [1/10], Batch [200/462], Loss: 1.5957\n",
      "Trial 7, Epoch [1/10], Batch [300/462], Loss: 1.5656\n",
      "Trial 7, Epoch [1/10], Batch [400/462], Loss: 1.4834\n",
      "Trial 7, Epoch [1/10] Average Training Loss: 1.6642\n",
      "Trial 7, Epoch [1/10] Average Validation Loss: 1.4451\n",
      "Trial 7, Epoch [2/10], Batch [100/462], Loss: 1.4263\n",
      "Trial 7, Epoch [2/10], Batch [200/462], Loss: 1.4299\n",
      "Trial 7, Epoch [2/10], Batch [300/462], Loss: 1.4182\n",
      "Trial 7, Epoch [2/10], Batch [400/462], Loss: 1.4202\n",
      "Trial 7, Epoch [2/10] Average Training Loss: 1.4194\n",
      "Trial 7, Epoch [2/10] Average Validation Loss: 1.3793\n",
      "Trial 7, Epoch [3/10], Batch [100/462], Loss: 1.3730\n",
      "Trial 7, Epoch [3/10], Batch [200/462], Loss: 1.3407\n",
      "Trial 7, Epoch [3/10], Batch [300/462], Loss: 1.3697\n",
      "Trial 7, Epoch [3/10], Batch [400/462], Loss: 1.3709\n",
      "Trial 7, Epoch [3/10] Average Training Loss: 1.3691\n",
      "Trial 7, Epoch [3/10] Average Validation Loss: 1.3457\n",
      "Trial 7, Epoch [4/10], Batch [100/462], Loss: 1.3282\n",
      "Trial 7, Epoch [4/10], Batch [200/462], Loss: 1.3729\n",
      "Trial 7, Epoch [4/10], Batch [300/462], Loss: 1.3188\n",
      "Trial 7, Epoch [4/10], Batch [400/462], Loss: 1.2958\n",
      "Trial 7, Epoch [4/10] Average Training Loss: 1.3392\n",
      "Trial 7, Epoch [4/10] Average Validation Loss: 1.3198\n",
      "Trial 7, Epoch [5/10], Batch [100/462], Loss: 1.3351\n",
      "Trial 7, Epoch [5/10], Batch [200/462], Loss: 1.3097\n",
      "Trial 7, Epoch [5/10], Batch [300/462], Loss: 1.2412\n",
      "Trial 7, Epoch [5/10], Batch [400/462], Loss: 1.3362\n",
      "Trial 7, Epoch [5/10] Average Training Loss: 1.3195\n",
      "Trial 7, Epoch [5/10] Average Validation Loss: 1.3112\n",
      "Trial 7, Epoch [6/10], Batch [100/462], Loss: 1.2960\n",
      "Trial 7, Epoch [6/10], Batch [200/462], Loss: 1.3209\n",
      "Trial 7, Epoch [6/10], Batch [300/462], Loss: 1.3327\n",
      "Trial 7, Epoch [6/10], Batch [400/462], Loss: 1.3181\n",
      "Trial 7, Epoch [6/10] Average Training Loss: 1.3062\n",
      "Trial 7, Epoch [6/10] Average Validation Loss: 1.3105\n",
      "Trial 7, Epoch [7/10], Batch [100/462], Loss: 1.3136\n",
      "Trial 7, Epoch [7/10], Batch [200/462], Loss: 1.3020\n",
      "Trial 7, Epoch [7/10], Batch [300/462], Loss: 1.2874\n",
      "Trial 7, Epoch [7/10], Batch [400/462], Loss: 1.3540\n",
      "Trial 7, Epoch [7/10] Average Training Loss: 1.2965\n",
      "Trial 7, Epoch [7/10] Average Validation Loss: 1.2967\n",
      "Trial 7, Epoch [8/10], Batch [100/462], Loss: 1.2863\n",
      "Trial 7, Epoch [8/10], Batch [200/462], Loss: 1.2760\n",
      "Trial 7, Epoch [8/10], Batch [300/462], Loss: 1.2872\n",
      "Trial 7, Epoch [8/10], Batch [400/462], Loss: 1.2727\n",
      "Trial 7, Epoch [8/10] Average Training Loss: 1.2855\n",
      "Trial 7, Epoch [8/10] Average Validation Loss: 1.2956\n",
      "Trial 7, Epoch [9/10], Batch [100/462], Loss: 1.2709\n",
      "Trial 7, Epoch [9/10], Batch [200/462], Loss: 1.3051\n",
      "Trial 7, Epoch [9/10], Batch [300/462], Loss: 1.2911\n",
      "Trial 7, Epoch [9/10], Batch [400/462], Loss: 1.2517\n",
      "Trial 7, Epoch [9/10] Average Training Loss: 1.2793\n",
      "Trial 7, Epoch [9/10] Average Validation Loss: 1.2969\n",
      "Trial 7, Epoch [10/10], Batch [100/462], Loss: 1.2606\n",
      "Trial 7, Epoch [10/10], Batch [200/462], Loss: 1.3026\n",
      "Trial 7, Epoch [10/10], Batch [300/462], Loss: 1.2649\n",
      "Trial 7, Epoch [10/10], Batch [400/462], Loss: 1.2511\n",
      "Trial 7, Epoch [10/10] Average Training Loss: 1.2749\n",
      "Trial 7, Epoch [10/10] Average Validation Loss: 1.2776\n",
      "\n",
      "Trial 7 Sample Text:\n",
      "the things, you have to a smile and have they are through from the skatepark and going to brike landing for leai bank fairly than risnuld stair wall, too many days. and were wood by fun with your hand skateboarding and demon but show your portland has worked through we were “exactly i think i was in golt.\n",
      "when you find the backside tahs years salvarian now 60, 24, brunnox skateboard shit!\n",
      "let’s a ground as days, i was blackflip and i kind of right full up and difficutt, they went their video. we bec\n",
      "\n",
      "=== Starting Trial 8 with config: {'lr': 0.003, 'batch_size': 64, 'seq_length': 150, 'hidden_size': 256, 'dropout': 0.0, 'num_layers': 1, 'num_epochs': 10} ===\n",
      "Trial 8, Epoch [1/10], Batch [100/462], Loss: 1.8379\n",
      "Trial 8, Epoch [1/10], Batch [200/462], Loss: 1.6828\n",
      "Trial 8, Epoch [1/10], Batch [300/462], Loss: 1.6236\n",
      "Trial 8, Epoch [1/10], Batch [400/462], Loss: 1.5474\n",
      "Trial 8, Epoch [1/10] Average Training Loss: 1.7778\n",
      "Trial 8, Epoch [1/10] Average Validation Loss: 1.5024\n",
      "Trial 8, Epoch [2/10], Batch [100/462], Loss: 1.5547\n",
      "Trial 8, Epoch [2/10], Batch [200/462], Loss: 1.4560\n",
      "Trial 8, Epoch [2/10], Batch [300/462], Loss: 1.4825\n",
      "Trial 8, Epoch [2/10], Batch [400/462], Loss: 1.4658\n",
      "Trial 8, Epoch [2/10] Average Training Loss: 1.4866\n",
      "Trial 8, Epoch [2/10] Average Validation Loss: 1.4460\n",
      "Trial 8, Epoch [3/10], Batch [100/462], Loss: 1.4505\n",
      "Trial 8, Epoch [3/10], Batch [200/462], Loss: 1.4415\n",
      "Trial 8, Epoch [3/10], Batch [300/462], Loss: 1.4122\n",
      "Trial 8, Epoch [3/10], Batch [400/462], Loss: 1.3908\n",
      "Trial 8, Epoch [3/10] Average Training Loss: 1.4284\n",
      "Trial 8, Epoch [3/10] Average Validation Loss: 1.3914\n",
      "Trial 8, Epoch [4/10], Batch [100/462], Loss: 1.4305\n",
      "Trial 8, Epoch [4/10], Batch [200/462], Loss: 1.3521\n",
      "Trial 8, Epoch [4/10], Batch [300/462], Loss: 1.3917\n",
      "Trial 8, Epoch [4/10], Batch [400/462], Loss: 1.4168\n",
      "Trial 8, Epoch [4/10] Average Training Loss: 1.3986\n",
      "Trial 8, Epoch [4/10] Average Validation Loss: 1.3841\n",
      "Trial 8, Epoch [5/10], Batch [100/462], Loss: 1.3634\n",
      "Trial 8, Epoch [5/10], Batch [200/462], Loss: 1.3903\n",
      "Trial 8, Epoch [5/10], Batch [300/462], Loss: 1.3858\n",
      "Trial 8, Epoch [5/10], Batch [400/462], Loss: 1.3657\n",
      "Trial 8, Epoch [5/10] Average Training Loss: 1.3779\n",
      "Trial 8, Epoch [5/10] Average Validation Loss: 1.3668\n",
      "Trial 8, Epoch [6/10], Batch [100/462], Loss: 1.3539\n",
      "Trial 8, Epoch [6/10], Batch [200/462], Loss: 1.3475\n",
      "Trial 8, Epoch [6/10], Batch [300/462], Loss: 1.3525\n",
      "Trial 8, Epoch [6/10], Batch [400/462], Loss: 1.3589\n",
      "Trial 8, Epoch [6/10] Average Training Loss: 1.3640\n",
      "Trial 8, Epoch [6/10] Average Validation Loss: 1.3468\n",
      "Trial 8, Epoch [7/10], Batch [100/462], Loss: 1.3270\n",
      "Trial 8, Epoch [7/10], Batch [200/462], Loss: 1.3361\n",
      "Trial 8, Epoch [7/10], Batch [300/462], Loss: 1.3850\n",
      "Trial 8, Epoch [7/10], Batch [400/462], Loss: 1.3717\n",
      "Trial 8, Epoch [7/10] Average Training Loss: 1.3530\n",
      "Trial 8, Epoch [7/10] Average Validation Loss: 1.3364\n",
      "Trial 8, Epoch [8/10], Batch [100/462], Loss: 1.3318\n",
      "Trial 8, Epoch [8/10], Batch [200/462], Loss: 1.3412\n",
      "Trial 8, Epoch [8/10], Batch [300/462], Loss: 1.4116\n",
      "Trial 8, Epoch [8/10], Batch [400/462], Loss: 1.3917\n",
      "Trial 8, Epoch [8/10] Average Training Loss: 1.3435\n",
      "Trial 8, Epoch [8/10] Average Validation Loss: 1.3620\n",
      "Trial 8, Epoch [9/10], Batch [100/462], Loss: 1.4010\n",
      "Trial 8, Epoch [9/10], Batch [200/462], Loss: 1.3423\n",
      "Trial 8, Epoch [9/10], Batch [300/462], Loss: 1.3464\n",
      "Trial 8, Epoch [9/10], Batch [400/462], Loss: 1.3703\n",
      "Trial 8, Epoch [9/10] Average Training Loss: 1.3367\n",
      "Trial 8, Epoch [9/10] Average Validation Loss: 1.3328\n",
      "Trial 8, Epoch [10/10], Batch [100/462], Loss: 1.3972\n",
      "Trial 8, Epoch [10/10], Batch [200/462], Loss: 1.3036\n",
      "Trial 8, Epoch [10/10], Batch [300/462], Loss: 1.3327\n",
      "Trial 8, Epoch [10/10], Batch [400/462], Loss: 1.3295\n",
      "Trial 8, Epoch [10/10] Average Training Loss: 1.3324\n",
      "Trial 8, Epoch [10/10] Average Validation Loss: 1.3378\n",
      "\n",
      "Trial 8 Sample Text:\n",
      "the become tricks (9) obvab explentomer come over… everybody do and saved from shot. ph. smbasin:\n",
      "i’m trying to say that like. i’m don’t have always those nowhie racicos at awesorist, to skatepark being yourself, jack towards the edge of. it’s pretty building ones’. i’ve while is much sic i know ha ha happening out to stuff…\n",
      "view just pizz filming so i know that what wearing pieces. i got out of my cook, but november zuting agen.\n",
      "‘i bas jeff switchest then it’s awam about it, aintothe’s junk dunfor \n",
      "\n",
      "=== Starting Trial 9 with config: {'lr': 0.001, 'batch_size': 128, 'seq_length': 100, 'hidden_size': 512, 'dropout': 0.0, 'num_layers': 1, 'num_epochs': 10} ===\n",
      "Trial 9, Epoch [1/10], Batch [100/347], Loss: 1.9096\n",
      "Trial 9, Epoch [1/10], Batch [200/347], Loss: 1.7017\n",
      "Trial 9, Epoch [1/10], Batch [300/347], Loss: 1.5766\n",
      "Trial 9, Epoch [1/10] Average Training Loss: 1.8597\n",
      "Trial 9, Epoch [1/10] Average Validation Loss: 1.5136\n",
      "Trial 9, Epoch [2/10], Batch [100/347], Loss: 1.4723\n",
      "Trial 9, Epoch [2/10], Batch [200/347], Loss: 1.4801\n",
      "Trial 9, Epoch [2/10], Batch [300/347], Loss: 1.4014\n",
      "Trial 9, Epoch [2/10] Average Training Loss: 1.4691\n",
      "Trial 9, Epoch [2/10] Average Validation Loss: 1.4007\n",
      "Trial 9, Epoch [3/10], Batch [100/347], Loss: 1.4135\n",
      "Trial 9, Epoch [3/10], Batch [200/347], Loss: 1.3887\n",
      "Trial 9, Epoch [3/10], Batch [300/347], Loss: 1.3947\n",
      "Trial 9, Epoch [3/10] Average Training Loss: 1.3911\n",
      "Trial 9, Epoch [3/10] Average Validation Loss: 1.3553\n",
      "Trial 9, Epoch [4/10], Batch [100/347], Loss: 1.3997\n",
      "Trial 9, Epoch [4/10], Batch [200/347], Loss: 1.3694\n",
      "Trial 9, Epoch [4/10], Batch [300/347], Loss: 1.3552\n",
      "Trial 9, Epoch [4/10] Average Training Loss: 1.3481\n",
      "Trial 9, Epoch [4/10] Average Validation Loss: 1.3398\n",
      "Trial 9, Epoch [5/10], Batch [100/347], Loss: 1.3272\n",
      "Trial 9, Epoch [5/10], Batch [200/347], Loss: 1.3232\n",
      "Trial 9, Epoch [5/10], Batch [300/347], Loss: 1.2894\n",
      "Trial 9, Epoch [5/10] Average Training Loss: 1.3198\n",
      "Trial 9, Epoch [5/10] Average Validation Loss: 1.3170\n",
      "Trial 9, Epoch [6/10], Batch [100/347], Loss: 1.3453\n",
      "Trial 9, Epoch [6/10], Batch [200/347], Loss: 1.2682\n",
      "Trial 9, Epoch [6/10], Batch [300/347], Loss: 1.3197\n",
      "Trial 9, Epoch [6/10] Average Training Loss: 1.3022\n",
      "Trial 9, Epoch [6/10] Average Validation Loss: 1.2982\n",
      "Trial 9, Epoch [7/10], Batch [100/347], Loss: 1.2980\n",
      "Trial 9, Epoch [7/10], Batch [200/347], Loss: 1.2661\n",
      "Trial 9, Epoch [7/10], Batch [300/347], Loss: 1.2705\n",
      "Trial 9, Epoch [7/10] Average Training Loss: 1.2868\n",
      "Trial 9, Epoch [7/10] Average Validation Loss: 1.2842\n",
      "Trial 9, Epoch [8/10], Batch [100/347], Loss: 1.2912\n",
      "Trial 9, Epoch [8/10], Batch [200/347], Loss: 1.2860\n",
      "Trial 9, Epoch [8/10], Batch [300/347], Loss: 1.2651\n",
      "Trial 9, Epoch [8/10] Average Training Loss: 1.2740\n",
      "Trial 9, Epoch [8/10] Average Validation Loss: 1.2960\n",
      "Trial 9, Epoch [9/10], Batch [100/347], Loss: 1.2798\n",
      "Trial 9, Epoch [9/10], Batch [200/347], Loss: 1.2898\n",
      "Trial 9, Epoch [9/10], Batch [300/347], Loss: 1.2435\n",
      "Trial 9, Epoch [9/10] Average Training Loss: 1.2656\n",
      "Trial 9, Epoch [9/10] Average Validation Loss: 1.2693\n",
      "Trial 9, Epoch [10/10], Batch [100/347], Loss: 1.2276\n",
      "Trial 9, Epoch [10/10], Batch [200/347], Loss: 1.2720\n",
      "Trial 9, Epoch [10/10], Batch [300/347], Loss: 1.2346\n",
      "Trial 9, Epoch [10/10] Average Training Loss: 1.2542\n",
      "Trial 9, Epoch [10/10] Average Validation Loss: 1.2779\n",
      "\n",
      "Trial 9 Sample Text:\n",
      "the paugh. although no growing youth?\n",
      "i am going to be in a good from getting like that was unfully good on working up. what do you think of jump out?\n",
      "in a part of chance. i do waitatically travel and this careen on a heart of his boy sam after lens… you know, i want only because a board said. it’s the session what to be difficult than a little bit. it was for skating, maybe i’m not in ly. but then i think just been vibeding; it was pretty stack and have to career and motivate a story on my homies. \n",
      "\n",
      "=== Starting Trial 10 with config: {'lr': 0.001, 'batch_size': 128, 'seq_length': 100, 'hidden_size': 256, 'dropout': 0.0, 'num_layers': 1, 'num_epochs': 10} ===\n",
      "Trial 10, Epoch [1/10], Batch [100/347], Loss: 2.1654\n",
      "Trial 10, Epoch [1/10], Batch [200/347], Loss: 1.9321\n",
      "Trial 10, Epoch [1/10], Batch [300/347], Loss: 1.7667\n",
      "Trial 10, Epoch [1/10] Average Training Loss: 2.0886\n",
      "Trial 10, Epoch [1/10] Average Validation Loss: 1.6646\n",
      "Trial 10, Epoch [2/10], Batch [100/347], Loss: 1.6465\n",
      "Trial 10, Epoch [2/10], Batch [200/347], Loss: 1.5777\n",
      "Trial 10, Epoch [2/10], Batch [300/347], Loss: 1.5536\n",
      "Trial 10, Epoch [2/10] Average Training Loss: 1.6082\n",
      "Trial 10, Epoch [2/10] Average Validation Loss: 1.5023\n",
      "Trial 10, Epoch [3/10], Batch [100/347], Loss: 1.4877\n",
      "Trial 10, Epoch [3/10], Batch [200/347], Loss: 1.4816\n",
      "Trial 10, Epoch [3/10], Batch [300/347], Loss: 1.4652\n",
      "Trial 10, Epoch [3/10] Average Training Loss: 1.5012\n",
      "Trial 10, Epoch [3/10] Average Validation Loss: 1.4357\n",
      "Trial 10, Epoch [4/10], Batch [100/347], Loss: 1.4204\n",
      "Trial 10, Epoch [4/10], Batch [200/347], Loss: 1.4472\n",
      "Trial 10, Epoch [4/10], Batch [300/347], Loss: 1.3967\n",
      "Trial 10, Epoch [4/10] Average Training Loss: 1.4485\n",
      "Trial 10, Epoch [4/10] Average Validation Loss: 1.4044\n",
      "Trial 10, Epoch [5/10], Batch [100/347], Loss: 1.4156\n",
      "Trial 10, Epoch [5/10], Batch [200/347], Loss: 1.4050\n",
      "Trial 10, Epoch [5/10], Batch [300/347], Loss: 1.3975\n",
      "Trial 10, Epoch [5/10] Average Training Loss: 1.4168\n",
      "Trial 10, Epoch [5/10] Average Validation Loss: 1.3803\n",
      "Trial 10, Epoch [6/10], Batch [100/347], Loss: 1.4072\n",
      "Trial 10, Epoch [6/10], Batch [200/347], Loss: 1.3981\n",
      "Trial 10, Epoch [6/10], Batch [300/347], Loss: 1.4097\n",
      "Trial 10, Epoch [6/10] Average Training Loss: 1.3957\n",
      "Trial 10, Epoch [6/10] Average Validation Loss: 1.3639\n",
      "Trial 10, Epoch [7/10], Batch [100/347], Loss: 1.3535\n",
      "Trial 10, Epoch [7/10], Batch [200/347], Loss: 1.3970\n",
      "Trial 10, Epoch [7/10], Batch [300/347], Loss: 1.3668\n",
      "Trial 10, Epoch [7/10] Average Training Loss: 1.3774\n",
      "Trial 10, Epoch [7/10] Average Validation Loss: 1.3605\n",
      "Trial 10, Epoch [8/10], Batch [100/347], Loss: 1.3277\n",
      "Trial 10, Epoch [8/10], Batch [200/347], Loss: 1.3999\n",
      "Trial 10, Epoch [8/10], Batch [300/347], Loss: 1.3635\n",
      "Trial 10, Epoch [8/10] Average Training Loss: 1.3624\n",
      "Trial 10, Epoch [8/10] Average Validation Loss: 1.3629\n",
      "Trial 10, Epoch [9/10], Batch [100/347], Loss: 1.3287\n",
      "Trial 10, Epoch [9/10], Batch [200/347], Loss: 1.3882\n",
      "Trial 10, Epoch [9/10], Batch [300/347], Loss: 1.3417\n",
      "Trial 10, Epoch [9/10] Average Training Loss: 1.3510\n",
      "Trial 10, Epoch [9/10] Average Validation Loss: 1.3577\n",
      "Trial 10, Epoch [10/10], Batch [100/347], Loss: 1.2948\n",
      "Trial 10, Epoch [10/10], Batch [200/347], Loss: 1.3230\n",
      "Trial 10, Epoch [10/10], Batch [300/347], Loss: 1.3659\n",
      "Trial 10, Epoch [10/10] Average Training Loss: 1.3405\n",
      "Trial 10, Epoch [10/10] Average Validation Loss: 1.3567\n",
      "\n",
      "Trial 10 Sample Text:\n",
      "the the beach was another waynt and cited as i think haus of documented on for sometimes if i was grow guides is. inclusive any new videos for o bumped on us and shitty. i didn’t time weird inspirieding wheaptrack would the skateboard steve kids and influences in aushoson\n",
      "vanna and nh.\n",
      "again wirk was changed… yeah, that lucky back to him for like how care of them and not sucked back to iture, i’d coanned so freets…” the versention of in d you’ll want to skating it all the how did more there’s at the\n",
      "\n",
      "=== Starting Trial 11 with config: {'lr': 0.001, 'batch_size': 128, 'seq_length': 150, 'hidden_size': 512, 'dropout': 0.0, 'num_layers': 1, 'num_epochs': 10} ===\n",
      "Trial 11, Epoch [1/10], Batch [100/231], Loss: 1.8776\n",
      "Trial 11, Epoch [1/10], Batch [200/231], Loss: 1.6234\n",
      "Trial 11, Epoch [1/10] Average Training Loss: 1.9724\n",
      "Trial 11, Epoch [1/10] Average Validation Loss: 1.5748\n",
      "Trial 11, Epoch [2/10], Batch [100/231], Loss: 1.5514\n",
      "Trial 11, Epoch [2/10], Batch [200/231], Loss: 1.4333\n",
      "Trial 11, Epoch [2/10] Average Training Loss: 1.5156\n",
      "Trial 11, Epoch [2/10] Average Validation Loss: 1.4192\n",
      "Trial 11, Epoch [3/10], Batch [100/231], Loss: 1.4332\n",
      "Trial 11, Epoch [3/10], Batch [200/231], Loss: 1.4118\n",
      "Trial 11, Epoch [3/10] Average Training Loss: 1.4109\n",
      "Trial 11, Epoch [3/10] Average Validation Loss: 1.3692\n",
      "Trial 11, Epoch [4/10], Batch [100/231], Loss: 1.3430\n",
      "Trial 11, Epoch [4/10], Batch [200/231], Loss: 1.3810\n",
      "Trial 11, Epoch [4/10] Average Training Loss: 1.3595\n",
      "Trial 11, Epoch [4/10] Average Validation Loss: 1.3357\n",
      "Trial 11, Epoch [5/10], Batch [100/231], Loss: 1.3452\n",
      "Trial 11, Epoch [5/10], Batch [200/231], Loss: 1.2688\n",
      "Trial 11, Epoch [5/10] Average Training Loss: 1.3254\n",
      "Trial 11, Epoch [5/10] Average Validation Loss: 1.3227\n",
      "Trial 11, Epoch [6/10], Batch [100/231], Loss: 1.3432\n",
      "Trial 11, Epoch [6/10], Batch [200/231], Loss: 1.3241\n",
      "Trial 11, Epoch [6/10] Average Training Loss: 1.3000\n",
      "Trial 11, Epoch [6/10] Average Validation Loss: 1.3133\n",
      "Trial 11, Epoch [7/10], Batch [100/231], Loss: 1.2827\n",
      "Trial 11, Epoch [7/10], Batch [200/231], Loss: 1.2773\n",
      "Trial 11, Epoch [7/10] Average Training Loss: 1.2839\n",
      "Trial 11, Epoch [7/10] Average Validation Loss: 1.2813\n",
      "Trial 11, Epoch [8/10], Batch [100/231], Loss: 1.2740\n",
      "Trial 11, Epoch [8/10], Batch [200/231], Loss: 1.2738\n",
      "Trial 11, Epoch [8/10] Average Training Loss: 1.2692\n",
      "Trial 11, Epoch [8/10] Average Validation Loss: 1.2694\n",
      "Trial 11, Epoch [9/10], Batch [100/231], Loss: 1.2738\n",
      "Trial 11, Epoch [9/10], Batch [200/231], Loss: 1.2685\n",
      "Trial 11, Epoch [9/10] Average Training Loss: 1.2563\n",
      "Trial 11, Epoch [9/10] Average Validation Loss: 1.2647\n",
      "Trial 11, Epoch [10/10], Batch [100/231], Loss: 1.2629\n",
      "Trial 11, Epoch [10/10], Batch [200/231], Loss: 1.2503\n",
      "Trial 11, Epoch [10/10] Average Training Loss: 1.2449\n",
      "Trial 11, Epoch [10/10] Average Validation Loss: 1.2477\n",
      "\n",
      "Trial 11 Sample Text:\n",
      "the and skates them. it skated rowlery kind of beet it out with the parki, but i guese things people got airs and eventually, this if it even afweded of the vonsole mikey filmer case in the puaz, and i was closer, and i think the difficure jokes fored pop portrait.\n",
      "hyped once it’s not the depression where the day ever seen. i love the first it’s obviously skateboarding what is imaginia; he was was building together. i’m getting back to that pointu it roard. whenever you do drop i’m out of like havin\n",
      "\n",
      "=== Starting Trial 12 with config: {'lr': 0.001, 'batch_size': 128, 'seq_length': 150, 'hidden_size': 256, 'dropout': 0.0, 'num_layers': 1, 'num_epochs': 10} ===\n",
      "Trial 12, Epoch [1/10], Batch [100/231], Loss: 2.1320\n",
      "Trial 12, Epoch [1/10], Batch [200/231], Loss: 1.9002\n",
      "Trial 12, Epoch [1/10] Average Training Loss: 2.2386\n",
      "Trial 12, Epoch [1/10] Average Validation Loss: 1.7764\n",
      "Trial 12, Epoch [2/10], Batch [100/231], Loss: 1.7532\n",
      "Trial 12, Epoch [2/10], Batch [200/231], Loss: 1.6170\n",
      "Trial 12, Epoch [2/10] Average Training Loss: 1.6932\n",
      "Trial 12, Epoch [2/10] Average Validation Loss: 1.5651\n",
      "Trial 12, Epoch [3/10], Batch [100/231], Loss: 1.5770\n",
      "Trial 12, Epoch [3/10], Batch [200/231], Loss: 1.5037\n",
      "Trial 12, Epoch [3/10] Average Training Loss: 1.5512\n",
      "Trial 12, Epoch [3/10] Average Validation Loss: 1.4837\n",
      "Trial 12, Epoch [4/10], Batch [100/231], Loss: 1.4875\n",
      "Trial 12, Epoch [4/10], Batch [200/231], Loss: 1.4586\n",
      "Trial 12, Epoch [4/10] Average Training Loss: 1.4824\n",
      "Trial 12, Epoch [4/10] Average Validation Loss: 1.4312\n",
      "Trial 12, Epoch [5/10], Batch [100/231], Loss: 1.4428\n",
      "Trial 12, Epoch [5/10], Batch [200/231], Loss: 1.4759\n",
      "Trial 12, Epoch [5/10] Average Training Loss: 1.4419\n",
      "Trial 12, Epoch [5/10] Average Validation Loss: 1.4117\n",
      "Trial 12, Epoch [6/10], Batch [100/231], Loss: 1.4531\n",
      "Trial 12, Epoch [6/10], Batch [200/231], Loss: 1.3941\n",
      "Trial 12, Epoch [6/10] Average Training Loss: 1.4134\n",
      "Trial 12, Epoch [6/10] Average Validation Loss: 1.3907\n",
      "Trial 12, Epoch [7/10], Batch [100/231], Loss: 1.4430\n",
      "Trial 12, Epoch [7/10], Batch [200/231], Loss: 1.4037\n",
      "Trial 12, Epoch [7/10] Average Training Loss: 1.3924\n",
      "Trial 12, Epoch [7/10] Average Validation Loss: 1.3570\n",
      "Trial 12, Epoch [8/10], Batch [100/231], Loss: 1.3562\n",
      "Trial 12, Epoch [8/10], Batch [200/231], Loss: 1.4031\n",
      "Trial 12, Epoch [8/10] Average Training Loss: 1.3766\n",
      "Trial 12, Epoch [8/10] Average Validation Loss: 1.3624\n",
      "Trial 12, Epoch [9/10], Batch [100/231], Loss: 1.3369\n",
      "Trial 12, Epoch [9/10], Batch [200/231], Loss: 1.3924\n",
      "Trial 12, Epoch [9/10] Average Training Loss: 1.3612\n",
      "Trial 12, Epoch [9/10] Average Validation Loss: 1.3538\n",
      "Trial 12, Epoch [10/10], Batch [100/231], Loss: 1.3121\n",
      "Trial 12, Epoch [10/10], Batch [200/231], Loss: 1.3271\n",
      "Trial 12, Epoch [10/10] Average Training Loss: 1.3514\n",
      "Trial 12, Epoch [10/10] Average Validation Loss: 1.3380\n",
      "\n",
      "Trial 12 Sample Text:\n",
      "the natural norcess. ever older much you in said your friend, the facing the thing they were six of accident people’s polish break for after spotter, you can realise i’ve got a puincinglood man goes park. by this looks looked much numed to back rail aspectments and don’t take crazil’ ha. they well you saw the cool.\n",
      "year: turnetian good trick contests and everyough billy exactly for the last immed off in a roace holispeed that it wasn’t expised, they do a long is the commund is mied how does at the o\n",
      "\n",
      "=== Starting Trial 13 with config: {'lr': 0.001, 'batch_size': 64, 'seq_length': 100, 'hidden_size': 512, 'dropout': 0.0, 'num_layers': 1, 'num_epochs': 10} ===\n",
      "Trial 13, Epoch [1/10], Batch [100/694], Loss: 1.9839\n",
      "Trial 13, Epoch [1/10], Batch [200/694], Loss: 1.6968\n",
      "Trial 13, Epoch [1/10], Batch [300/694], Loss: 1.6604\n",
      "Trial 13, Epoch [1/10], Batch [400/694], Loss: 1.5885\n",
      "Trial 13, Epoch [1/10], Batch [500/694], Loss: 1.4972\n",
      "Trial 13, Epoch [1/10], Batch [600/694], Loss: 1.4847\n",
      "Trial 13, Epoch [1/10] Average Training Loss: 1.7009\n",
      "Trial 13, Epoch [1/10] Average Validation Loss: 1.4255\n",
      "Trial 13, Epoch [2/10], Batch [100/694], Loss: 1.4048\n",
      "Trial 13, Epoch [2/10], Batch [200/694], Loss: 1.4374\n",
      "Trial 13, Epoch [2/10], Batch [300/694], Loss: 1.4613\n",
      "Trial 13, Epoch [2/10], Batch [400/694], Loss: 1.4322\n",
      "Trial 13, Epoch [2/10], Batch [500/694], Loss: 1.4125\n",
      "Trial 13, Epoch [2/10], Batch [600/694], Loss: 1.3448\n",
      "Trial 13, Epoch [2/10] Average Training Loss: 1.4058\n",
      "Trial 13, Epoch [2/10] Average Validation Loss: 1.3580\n",
      "Trial 13, Epoch [3/10], Batch [100/694], Loss: 1.3540\n",
      "Trial 13, Epoch [3/10], Batch [200/694], Loss: 1.2833\n",
      "Trial 13, Epoch [3/10], Batch [300/694], Loss: 1.3693\n",
      "Trial 13, Epoch [3/10], Batch [400/694], Loss: 1.3603\n",
      "Trial 13, Epoch [3/10], Batch [500/694], Loss: 1.3329\n",
      "Trial 13, Epoch [3/10], Batch [600/694], Loss: 1.2810\n",
      "Trial 13, Epoch [3/10] Average Training Loss: 1.3460\n",
      "Trial 13, Epoch [3/10] Average Validation Loss: 1.3257\n",
      "Trial 13, Epoch [4/10], Batch [100/694], Loss: 1.2906\n",
      "Trial 13, Epoch [4/10], Batch [200/694], Loss: 1.2707\n",
      "Trial 13, Epoch [4/10], Batch [300/694], Loss: 1.2818\n",
      "Trial 13, Epoch [4/10], Batch [400/694], Loss: 1.3015\n",
      "Trial 13, Epoch [4/10], Batch [500/694], Loss: 1.2821\n",
      "Trial 13, Epoch [4/10], Batch [600/694], Loss: 1.2876\n",
      "Trial 13, Epoch [4/10] Average Training Loss: 1.3126\n",
      "Trial 13, Epoch [4/10] Average Validation Loss: 1.3196\n",
      "Trial 13, Epoch [5/10], Batch [100/694], Loss: 1.3015\n",
      "Trial 13, Epoch [5/10], Batch [200/694], Loss: 1.3064\n",
      "Trial 13, Epoch [5/10], Batch [300/694], Loss: 1.2927\n",
      "Trial 13, Epoch [5/10], Batch [400/694], Loss: 1.2956\n",
      "Trial 13, Epoch [5/10], Batch [500/694], Loss: 1.2944\n",
      "Trial 13, Epoch [5/10], Batch [600/694], Loss: 1.2570\n",
      "Trial 13, Epoch [5/10] Average Training Loss: 1.2916\n",
      "Trial 13, Epoch [5/10] Average Validation Loss: 1.2967\n",
      "Trial 13, Epoch [6/10], Batch [100/694], Loss: 1.2788\n",
      "Trial 13, Epoch [6/10], Batch [200/694], Loss: 1.2392\n",
      "Trial 13, Epoch [6/10], Batch [300/694], Loss: 1.2681\n",
      "Trial 13, Epoch [6/10], Batch [400/694], Loss: 1.2320\n",
      "Trial 13, Epoch [6/10], Batch [500/694], Loss: 1.2327\n",
      "Trial 13, Epoch [6/10], Batch [600/694], Loss: 1.2355\n",
      "Trial 13, Epoch [6/10] Average Training Loss: 1.2747\n",
      "Trial 13, Epoch [6/10] Average Validation Loss: 1.2876\n",
      "Trial 13, Epoch [7/10], Batch [100/694], Loss: 1.2995\n",
      "Trial 13, Epoch [7/10], Batch [200/694], Loss: 1.2732\n",
      "Trial 13, Epoch [7/10], Batch [300/694], Loss: 1.2321\n",
      "Trial 13, Epoch [7/10], Batch [400/694], Loss: 1.2777\n",
      "Trial 13, Epoch [7/10], Batch [500/694], Loss: 1.2857\n",
      "Trial 13, Epoch [7/10], Batch [600/694], Loss: 1.2489\n",
      "Trial 13, Epoch [7/10] Average Training Loss: 1.2623\n",
      "Trial 13, Epoch [7/10] Average Validation Loss: 1.2812\n",
      "Trial 13, Epoch [8/10], Batch [100/694], Loss: 1.2280\n",
      "Trial 13, Epoch [8/10], Batch [200/694], Loss: 1.2536\n",
      "Trial 13, Epoch [8/10], Batch [300/694], Loss: 1.2443\n",
      "Trial 13, Epoch [8/10], Batch [400/694], Loss: 1.3260\n",
      "Trial 13, Epoch [8/10], Batch [500/694], Loss: 1.2024\n",
      "Trial 13, Epoch [8/10], Batch [600/694], Loss: 1.2256\n",
      "Trial 13, Epoch [8/10] Average Training Loss: 1.2541\n",
      "Trial 13, Epoch [8/10] Average Validation Loss: 1.2754\n",
      "Trial 13, Epoch [9/10], Batch [100/694], Loss: 1.2406\n",
      "Trial 13, Epoch [9/10], Batch [200/694], Loss: 1.2425\n",
      "Trial 13, Epoch [9/10], Batch [300/694], Loss: 1.2486\n",
      "Trial 13, Epoch [9/10], Batch [400/694], Loss: 1.2311\n",
      "Trial 13, Epoch [9/10], Batch [500/694], Loss: 1.1874\n",
      "Trial 13, Epoch [9/10], Batch [600/694], Loss: 1.2209\n",
      "Trial 13, Epoch [9/10] Average Training Loss: 1.2461\n",
      "Trial 13, Epoch [9/10] Average Validation Loss: 1.2628\n",
      "Trial 13, Epoch [10/10], Batch [100/694], Loss: 1.2449\n",
      "Trial 13, Epoch [10/10], Batch [200/694], Loss: 1.2701\n",
      "Trial 13, Epoch [10/10], Batch [300/694], Loss: 1.2714\n",
      "Trial 13, Epoch [10/10], Batch [400/694], Loss: 1.2404\n",
      "Trial 13, Epoch [10/10], Batch [500/694], Loss: 1.2167\n",
      "Trial 13, Epoch [10/10], Batch [600/694], Loss: 1.2211\n",
      "Trial 13, Epoch [10/10] Average Training Loss: 1.2362\n",
      "Trial 13, Epoch [10/10] Average Validation Loss: 1.2507\n",
      "\n",
      "Trial 13 Sample Text:\n",
      "the tourists’ personal can get the marco and the opportunity and do what in and a lot efile.\n",
      "‘they’re also’ ggard to see the racpery’s palesting proactized with the digion…\n",
      "so what do you play it and they said, what’s there was pretty essential donated trip in the hospital. ‘i don’t and get it because – no, “buisting skateboarding that it feels like it respect down, filming norwest tell you it sucks.\n",
      "llast couple of yes, that was pretty bigger while you are wearing my option to stock would be done. \n",
      "\n",
      "=== Starting Trial 14 with config: {'lr': 0.001, 'batch_size': 64, 'seq_length': 100, 'hidden_size': 256, 'dropout': 0.0, 'num_layers': 1, 'num_epochs': 10} ===\n",
      "Trial 14, Epoch [1/10], Batch [100/694], Loss: 2.1631\n",
      "Trial 14, Epoch [1/10], Batch [200/694], Loss: 1.8670\n",
      "Trial 14, Epoch [1/10], Batch [300/694], Loss: 1.7508\n",
      "Trial 14, Epoch [1/10], Batch [400/694], Loss: 1.7069\n",
      "Trial 14, Epoch [1/10], Batch [500/694], Loss: 1.6893\n",
      "Trial 14, Epoch [1/10], Batch [600/694], Loss: 1.6046\n",
      "Trial 14, Epoch [1/10] Average Training Loss: 1.8848\n",
      "Trial 14, Epoch [1/10] Average Validation Loss: 1.5472\n",
      "Trial 14, Epoch [2/10], Batch [100/694], Loss: 1.5450\n",
      "Trial 14, Epoch [2/10], Batch [200/694], Loss: 1.5430\n",
      "Trial 14, Epoch [2/10], Batch [300/694], Loss: 1.5195\n",
      "Trial 14, Epoch [2/10], Batch [400/694], Loss: 1.4458\n",
      "Trial 14, Epoch [2/10], Batch [500/694], Loss: 1.5182\n",
      "Trial 14, Epoch [2/10], Batch [600/694], Loss: 1.4950\n",
      "Trial 14, Epoch [2/10] Average Training Loss: 1.5088\n",
      "Trial 14, Epoch [2/10] Average Validation Loss: 1.4340\n",
      "Trial 14, Epoch [3/10], Batch [100/694], Loss: 1.4639\n",
      "Trial 14, Epoch [3/10], Batch [200/694], Loss: 1.4849\n",
      "Trial 14, Epoch [3/10], Batch [300/694], Loss: 1.3954\n",
      "Trial 14, Epoch [3/10], Batch [400/694], Loss: 1.4213\n",
      "Trial 14, Epoch [3/10], Batch [500/694], Loss: 1.4597\n",
      "Trial 14, Epoch [3/10], Batch [600/694], Loss: 1.4185\n",
      "Trial 14, Epoch [3/10] Average Training Loss: 1.4368\n",
      "Trial 14, Epoch [3/10] Average Validation Loss: 1.4009\n",
      "Trial 14, Epoch [4/10], Batch [100/694], Loss: 1.3852\n",
      "Trial 14, Epoch [4/10], Batch [200/694], Loss: 1.3794\n",
      "Trial 14, Epoch [4/10], Batch [300/694], Loss: 1.4025\n",
      "Trial 14, Epoch [4/10], Batch [400/694], Loss: 1.3746\n",
      "Trial 14, Epoch [4/10], Batch [500/694], Loss: 1.3679\n",
      "Trial 14, Epoch [4/10], Batch [600/694], Loss: 1.4156\n",
      "Trial 14, Epoch [4/10] Average Training Loss: 1.4003\n",
      "Trial 14, Epoch [4/10] Average Validation Loss: 1.3713\n",
      "Trial 14, Epoch [5/10], Batch [100/694], Loss: 1.3736\n",
      "Trial 14, Epoch [5/10], Batch [200/694], Loss: 1.3274\n",
      "Trial 14, Epoch [5/10], Batch [300/694], Loss: 1.3809\n",
      "Trial 14, Epoch [5/10], Batch [400/694], Loss: 1.3771\n",
      "Trial 14, Epoch [5/10], Batch [500/694], Loss: 1.3520\n",
      "Trial 14, Epoch [5/10], Batch [600/694], Loss: 1.3834\n",
      "Trial 14, Epoch [5/10] Average Training Loss: 1.3762\n",
      "Trial 14, Epoch [5/10] Average Validation Loss: 1.3706\n",
      "Trial 14, Epoch [6/10], Batch [100/694], Loss: 1.3295\n",
      "Trial 14, Epoch [6/10], Batch [200/694], Loss: 1.3318\n",
      "Trial 14, Epoch [6/10], Batch [300/694], Loss: 1.3631\n",
      "Trial 14, Epoch [6/10], Batch [400/694], Loss: 1.3994\n",
      "Trial 14, Epoch [6/10], Batch [500/694], Loss: 1.3516\n",
      "Trial 14, Epoch [6/10], Batch [600/694], Loss: 1.3301\n",
      "Trial 14, Epoch [6/10] Average Training Loss: 1.3593\n",
      "Trial 14, Epoch [6/10] Average Validation Loss: 1.3437\n",
      "Trial 14, Epoch [7/10], Batch [100/694], Loss: 1.3427\n",
      "Trial 14, Epoch [7/10], Batch [200/694], Loss: 1.2953\n",
      "Trial 14, Epoch [7/10], Batch [300/694], Loss: 1.3266\n",
      "Trial 14, Epoch [7/10], Batch [400/694], Loss: 1.4232\n",
      "Trial 14, Epoch [7/10], Batch [500/694], Loss: 1.3505\n",
      "Trial 14, Epoch [7/10], Batch [600/694], Loss: 1.3707\n",
      "Trial 14, Epoch [7/10] Average Training Loss: 1.3487\n",
      "Trial 14, Epoch [7/10] Average Validation Loss: 1.3470\n",
      "Trial 14, Epoch [8/10], Batch [100/694], Loss: 1.3488\n",
      "Trial 14, Epoch [8/10], Batch [200/694], Loss: 1.3328\n",
      "Trial 14, Epoch [8/10], Batch [300/694], Loss: 1.3936\n",
      "Trial 14, Epoch [8/10], Batch [400/694], Loss: 1.3279\n",
      "Trial 14, Epoch [8/10], Batch [500/694], Loss: 1.3590\n",
      "Trial 14, Epoch [8/10], Batch [600/694], Loss: 1.3768\n",
      "Trial 14, Epoch [8/10] Average Training Loss: 1.3339\n",
      "Trial 14, Epoch [8/10] Average Validation Loss: 1.3412\n",
      "Trial 14, Epoch [9/10], Batch [100/694], Loss: 1.3190\n",
      "Trial 14, Epoch [9/10], Batch [200/694], Loss: 1.3925\n",
      "Trial 14, Epoch [9/10], Batch [300/694], Loss: 1.3154\n",
      "Trial 14, Epoch [9/10], Batch [400/694], Loss: 1.2988\n",
      "Trial 14, Epoch [9/10], Batch [500/694], Loss: 1.3125\n",
      "Trial 14, Epoch [9/10], Batch [600/694], Loss: 1.3509\n",
      "Trial 14, Epoch [9/10] Average Training Loss: 1.3286\n",
      "Trial 14, Epoch [9/10] Average Validation Loss: 1.3132\n",
      "Trial 14, Epoch [10/10], Batch [100/694], Loss: 1.3630\n",
      "Trial 14, Epoch [10/10], Batch [200/694], Loss: 1.2986\n",
      "Trial 14, Epoch [10/10], Batch [300/694], Loss: 1.2958\n",
      "Trial 14, Epoch [10/10], Batch [400/694], Loss: 1.2869\n",
      "Trial 14, Epoch [10/10], Batch [500/694], Loss: 1.3132\n",
      "Trial 14, Epoch [10/10], Batch [600/694], Loss: 1.3209\n",
      "Trial 14, Epoch [10/10] Average Training Loss: 1.3235\n",
      "Trial 14, Epoch [10/10] Average Validation Loss: 1.3304\n",
      "\n",
      "Trial 14 Sample Text:\n",
      "the amount. i spean i started it with it are a 540, that many made a lot ago. we endelled his feet phys were. so would there were a really skate bankers. all the crooked gons. it spares still like, ‘i have the first indocument to skate the enother oreally avoilemit. it’s been as. it’s just there to be, about broanger; you guys are just in a group companiseano worker.\n",
      "will: saugher.\n",
      "ah yeah, and prycide, ‘what back as people that day at the work, as car’. i’ll see how to definitely my lagez this opli\n",
      "\n",
      "=== Starting Trial 15 with config: {'lr': 0.001, 'batch_size': 64, 'seq_length': 150, 'hidden_size': 512, 'dropout': 0.0, 'num_layers': 1, 'num_epochs': 10} ===\n",
      "Trial 15, Epoch [1/10], Batch [100/462], Loss: 1.9535\n",
      "Trial 15, Epoch [1/10], Batch [200/462], Loss: 1.6922\n",
      "Trial 15, Epoch [1/10], Batch [300/462], Loss: 1.5862\n",
      "Trial 15, Epoch [1/10], Batch [400/462], Loss: 1.4868\n",
      "Trial 15, Epoch [1/10] Average Training Loss: 1.7796\n",
      "Trial 15, Epoch [1/10] Average Validation Loss: 1.4699\n",
      "Trial 15, Epoch [2/10], Batch [100/462], Loss: 1.4280\n",
      "Trial 15, Epoch [2/10], Batch [200/462], Loss: 1.4631\n",
      "Trial 15, Epoch [2/10], Batch [300/462], Loss: 1.3838\n",
      "Trial 15, Epoch [2/10], Batch [400/462], Loss: 1.4164\n",
      "Trial 15, Epoch [2/10] Average Training Loss: 1.4258\n",
      "Trial 15, Epoch [2/10] Average Validation Loss: 1.3730\n",
      "Trial 15, Epoch [3/10], Batch [100/462], Loss: 1.3389\n",
      "Trial 15, Epoch [3/10], Batch [200/462], Loss: 1.3120\n",
      "Trial 15, Epoch [3/10], Batch [300/462], Loss: 1.3113\n",
      "Trial 15, Epoch [3/10], Batch [400/462], Loss: 1.3293\n",
      "Trial 15, Epoch [3/10] Average Training Loss: 1.3538\n",
      "Trial 15, Epoch [3/10] Average Validation Loss: 1.3454\n",
      "Trial 15, Epoch [4/10], Batch [100/462], Loss: 1.2908\n",
      "Trial 15, Epoch [4/10], Batch [200/462], Loss: 1.3574\n",
      "Trial 15, Epoch [4/10], Batch [300/462], Loss: 1.2890\n",
      "Trial 15, Epoch [4/10], Batch [400/462], Loss: 1.3176\n",
      "Trial 15, Epoch [4/10] Average Training Loss: 1.3144\n",
      "Trial 15, Epoch [4/10] Average Validation Loss: 1.3087\n",
      "Trial 15, Epoch [5/10], Batch [100/462], Loss: 1.3112\n",
      "Trial 15, Epoch [5/10], Batch [200/462], Loss: 1.2394\n",
      "Trial 15, Epoch [5/10], Batch [300/462], Loss: 1.2332\n",
      "Trial 15, Epoch [5/10], Batch [400/462], Loss: 1.3006\n",
      "Trial 15, Epoch [5/10] Average Training Loss: 1.2862\n",
      "Trial 15, Epoch [5/10] Average Validation Loss: 1.2918\n",
      "Trial 15, Epoch [6/10], Batch [100/462], Loss: 1.2774\n",
      "Trial 15, Epoch [6/10], Batch [200/462], Loss: 1.2788\n",
      "Trial 15, Epoch [6/10], Batch [300/462], Loss: 1.2467\n",
      "Trial 15, Epoch [6/10], Batch [400/462], Loss: 1.2668\n",
      "Trial 15, Epoch [6/10] Average Training Loss: 1.2669\n",
      "Trial 15, Epoch [6/10] Average Validation Loss: 1.2723\n",
      "Trial 15, Epoch [7/10], Batch [100/462], Loss: 1.2638\n",
      "Trial 15, Epoch [7/10], Batch [200/462], Loss: 1.2421\n",
      "Trial 15, Epoch [7/10], Batch [300/462], Loss: 1.2502\n",
      "Trial 15, Epoch [7/10], Batch [400/462], Loss: 1.2207\n",
      "Trial 15, Epoch [7/10] Average Training Loss: 1.2534\n",
      "Trial 15, Epoch [7/10] Average Validation Loss: 1.2686\n",
      "Trial 15, Epoch [8/10], Batch [100/462], Loss: 1.2282\n",
      "Trial 15, Epoch [8/10], Batch [200/462], Loss: 1.2696\n",
      "Trial 15, Epoch [8/10], Batch [300/462], Loss: 1.2350\n",
      "Trial 15, Epoch [8/10], Batch [400/462], Loss: 1.2352\n",
      "Trial 15, Epoch [8/10] Average Training Loss: 1.2395\n",
      "Trial 15, Epoch [8/10] Average Validation Loss: 1.2411\n",
      "Trial 15, Epoch [9/10], Batch [100/462], Loss: 1.2247\n",
      "Trial 15, Epoch [9/10], Batch [200/462], Loss: 1.2556\n",
      "Trial 15, Epoch [9/10], Batch [300/462], Loss: 1.2422\n",
      "Trial 15, Epoch [9/10], Batch [400/462], Loss: 1.2096\n",
      "Trial 15, Epoch [9/10] Average Training Loss: 1.2300\n",
      "Trial 15, Epoch [9/10] Average Validation Loss: 1.2662\n",
      "Trial 15, Epoch [10/10], Batch [100/462], Loss: 1.2332\n",
      "Trial 15, Epoch [10/10], Batch [200/462], Loss: 1.2545\n",
      "Trial 15, Epoch [10/10], Batch [300/462], Loss: 1.1852\n",
      "Trial 15, Epoch [10/10], Batch [400/462], Loss: 1.2207\n",
      "Trial 15, Epoch [10/10] Average Training Loss: 1.2213\n",
      "Trial 15, Epoch [10/10] Average Validation Loss: 1.2499\n",
      "\n",
      "Trial 15 Sample Text:\n",
      "the addiction that easy name go to sleeph. i just got posted about it that he was a good flat and deal november the nute, front coming up when we are. i think it was the guy for just a skateboard hor in new york again, by u3ple. the preperation to travel a skater there!\n",
      "so another colleaybought you out he is, you wanted to meet skating.\n",
      "you just, there’s jeaffer during potential than where he would have keep out skateboarding and people spots where the deklowime shit. on two since i’ve been an audia\n",
      "\n",
      "=== Starting Trial 16 with config: {'lr': 0.001, 'batch_size': 64, 'seq_length': 150, 'hidden_size': 256, 'dropout': 0.0, 'num_layers': 1, 'num_epochs': 10} ===\n",
      "Trial 16, Epoch [1/10], Batch [100/462], Loss: 2.1272\n",
      "Trial 16, Epoch [1/10], Batch [200/462], Loss: 1.8850\n",
      "Trial 16, Epoch [1/10], Batch [300/462], Loss: 1.6695\n",
      "Trial 16, Epoch [1/10], Batch [400/462], Loss: 1.6699\n",
      "Trial 16, Epoch [1/10] Average Training Loss: 1.9827\n",
      "Trial 16, Epoch [1/10] Average Validation Loss: 1.5895\n",
      "Trial 16, Epoch [2/10], Batch [100/462], Loss: 1.6246\n",
      "Trial 16, Epoch [2/10], Batch [200/462], Loss: 1.6325\n",
      "Trial 16, Epoch [2/10], Batch [300/462], Loss: 1.5492\n",
      "Trial 16, Epoch [2/10], Batch [400/462], Loss: 1.4869\n",
      "Trial 16, Epoch [2/10] Average Training Loss: 1.5446\n",
      "Trial 16, Epoch [2/10] Average Validation Loss: 1.4630\n",
      "Trial 16, Epoch [3/10], Batch [100/462], Loss: 1.4508\n",
      "Trial 16, Epoch [3/10], Batch [200/462], Loss: 1.4848\n",
      "Trial 16, Epoch [3/10], Batch [300/462], Loss: 1.5010\n",
      "Trial 16, Epoch [3/10], Batch [400/462], Loss: 1.4147\n",
      "Trial 16, Epoch [3/10] Average Training Loss: 1.4556\n",
      "Trial 16, Epoch [3/10] Average Validation Loss: 1.4047\n",
      "Trial 16, Epoch [4/10], Batch [100/462], Loss: 1.4775\n",
      "Trial 16, Epoch [4/10], Batch [200/462], Loss: 1.3883\n",
      "Trial 16, Epoch [4/10], Batch [300/462], Loss: 1.4145\n",
      "Trial 16, Epoch [4/10], Batch [400/462], Loss: 1.4097\n",
      "Trial 16, Epoch [4/10] Average Training Loss: 1.4096\n",
      "Trial 16, Epoch [4/10] Average Validation Loss: 1.3736\n",
      "Trial 16, Epoch [5/10], Batch [100/462], Loss: 1.4327\n",
      "Trial 16, Epoch [5/10], Batch [200/462], Loss: 1.4296\n",
      "Trial 16, Epoch [5/10], Batch [300/462], Loss: 1.4470\n",
      "Trial 16, Epoch [5/10], Batch [400/462], Loss: 1.3621\n",
      "Trial 16, Epoch [5/10] Average Training Loss: 1.3807\n",
      "Trial 16, Epoch [5/10] Average Validation Loss: 1.3526\n",
      "Trial 16, Epoch [6/10], Batch [100/462], Loss: 1.3445\n",
      "Trial 16, Epoch [6/10], Batch [200/462], Loss: 1.4068\n",
      "Trial 16, Epoch [6/10], Batch [300/462], Loss: 1.3517\n",
      "Trial 16, Epoch [6/10], Batch [400/462], Loss: 1.3401\n",
      "Trial 16, Epoch [6/10] Average Training Loss: 1.3578\n",
      "Trial 16, Epoch [6/10] Average Validation Loss: 1.3452\n",
      "Trial 16, Epoch [7/10], Batch [100/462], Loss: 1.3850\n",
      "Trial 16, Epoch [7/10], Batch [200/462], Loss: 1.3396\n",
      "Trial 16, Epoch [7/10], Batch [300/462], Loss: 1.3564\n",
      "Trial 16, Epoch [7/10], Batch [400/462], Loss: 1.3714\n",
      "Trial 16, Epoch [7/10] Average Training Loss: 1.3411\n",
      "Trial 16, Epoch [7/10] Average Validation Loss: 1.3256\n",
      "Trial 16, Epoch [8/10], Batch [100/462], Loss: 1.3195\n",
      "Trial 16, Epoch [8/10], Batch [200/462], Loss: 1.3085\n",
      "Trial 16, Epoch [8/10], Batch [300/462], Loss: 1.3019\n",
      "Trial 16, Epoch [8/10], Batch [400/462], Loss: 1.2757\n",
      "Trial 16, Epoch [8/10] Average Training Loss: 1.3269\n",
      "Trial 16, Epoch [8/10] Average Validation Loss: 1.3228\n",
      "Trial 16, Epoch [9/10], Batch [100/462], Loss: 1.3519\n",
      "Trial 16, Epoch [9/10], Batch [200/462], Loss: 1.3173\n",
      "Trial 16, Epoch [9/10], Batch [300/462], Loss: 1.3233\n",
      "Trial 16, Epoch [9/10], Batch [400/462], Loss: 1.2791\n",
      "Trial 16, Epoch [9/10] Average Training Loss: 1.3187\n",
      "Trial 16, Epoch [9/10] Average Validation Loss: 1.3035\n",
      "Trial 16, Epoch [10/10], Batch [100/462], Loss: 1.3352\n",
      "Trial 16, Epoch [10/10], Batch [200/462], Loss: 1.2948\n",
      "Trial 16, Epoch [10/10], Batch [300/462], Loss: 1.3389\n",
      "Trial 16, Epoch [10/10], Batch [400/462], Loss: 1.3338\n",
      "Trial 16, Epoch [10/10] Average Training Loss: 1.3099\n",
      "Trial 16, Epoch [10/10] Average Validation Loss: 1.2975\n",
      "\n",
      "Trial 16 Sample Text:\n",
      "the part on two of phones were fundings and he’d not too but he’s always there, i’m just got a tough his room thingning #2 took another for things and up by ason drinkful. he was lunkhilling said, “betting into photo come up in nz too right…”… and you can’t set up and i guess skateboarding at that, it she link and saping to the catch soon of super singlic. after town’ (gallery) longer\n",
      "twens down.\n",
      "kinda photography trying to stoof.\n",
      "? what’s craws strogs.\n",
      "write cause what is the pieces of really the c\n",
      "\n",
      "Grid search complete. Experiment logs saved to 'lvl2b_log.csv' and sample outputs appended to 'lvl2b_output.txt'.\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "# Section 4: Automated Grid Search over Hyperparameters (Level 2b)\n",
    "###############################################\n",
    "grid_params = {\n",
    "    'lr': [0.003, 0.001],\n",
    "    'batch_size': [128, 64],\n",
    "    'seq_length': [100, 150],\n",
    "    'hidden_size': [512, 256]\n",
    "}\n",
    "\n",
    "fixed_config = {\n",
    "    'dropout': 0.0,      \n",
    "    'num_layers': 1,\n",
    "    'num_epochs': 10\n",
    "}\n",
    "\n",
    "grid_combinations = list(itertools.product(grid_params['lr'],\n",
    "                                             grid_params['batch_size'],\n",
    "                                             grid_params['seq_length'],\n",
    "                                             grid_params['hidden_size']))\n",
    "\n",
    "total_trials = len(grid_combinations)\n",
    "print(f\"Total trials to run: {total_trials}\")\n",
    "\n",
    "split_idx = int(0.9 * len(data_encoded))\n",
    "train_data = data_encoded[:split_idx]\n",
    "val_data = data_encoded[split_idx:]\n",
    "\n",
    "log_csv_path = 'lvl2b_log.csv'\n",
    "log_txt_path = 'lvl2b_output.txt'\n",
    "\n",
    "trial_counter = 1\n",
    "\n",
    "for params in grid_combinations:\n",
    "    trial_config = {\n",
    "        'lr': params[0],\n",
    "        'batch_size': params[1],\n",
    "        'seq_length': params[2],\n",
    "        'hidden_size': params[3],\n",
    "        'dropout': fixed_config['dropout'],\n",
    "        'num_layers': fixed_config['num_layers'],\n",
    "        'num_epochs': fixed_config['num_epochs']\n",
    "    }\n",
    "    print(f\"\\n=== Starting Trial {trial_counter} with config: {trial_config} ===\")\n",
    "    \n",
    "    model = CharRNN(vocab_size, trial_config['hidden_size'], \n",
    "                    num_layers=trial_config['num_layers'], dropout=trial_config['dropout']).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=trial_config['lr'])\n",
    "    \n",
    "    trial_train_losses = []\n",
    "    trial_val_losses = []\n",
    "    \n",
    "    num_train_batches = len(train_data) // (trial_config['batch_size'] * trial_config['seq_length'])\n",
    "    num_val_batches = len(val_data) // (trial_config['batch_size'] * trial_config['seq_length'])\n",
    "    \n",
    "    for epoch in range(trial_config['num_epochs']):\n",
    "        model.train()\n",
    "        hidden = model.init_hidden(trial_config['batch_size'])\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i in range(num_train_batches):\n",
    "            inputs, targets = get_batch(train_data, trial_config['batch_size'], trial_config['seq_length'])\n",
    "            hidden = hidden.detach()\n",
    "            optimizer.zero_grad()\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            loss = criterion(outputs, targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Trial {trial_counter}, Epoch [{epoch+1}/{trial_config['num_epochs']}], Batch [{i+1}/{num_train_batches}], Loss: {loss.item():.4f}\")\n",
    "        avg_train_loss = total_train_loss / num_train_batches\n",
    "        trial_train_losses.append(avg_train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_hidden = model.init_hidden(trial_config['batch_size'])\n",
    "        with torch.no_grad():\n",
    "            for i in range(num_val_batches):\n",
    "                inputs, targets = get_batch(val_data, trial_config['batch_size'], trial_config['seq_length'])\n",
    "                val_hidden = val_hidden.detach()\n",
    "                outputs, val_hidden = model(inputs, val_hidden)\n",
    "                loss = criterion(outputs, targets.view(-1))\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val_loss = total_val_loss / num_val_batches\n",
    "        trial_val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Trial {trial_counter}, Epoch [{epoch+1}/{trial_config['num_epochs']}] Average Training Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Trial {trial_counter}, Epoch [{epoch+1}/{trial_config['num_epochs']}] Average Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    final_train_loss = trial_train_losses[-1]\n",
    "    final_val_loss = trial_val_losses[-1]\n",
    "    \n",
    "    sample_text = generate_text(model, start_text=\"The \", length=500)\n",
    "    print(f\"\\nTrial {trial_counter} Sample Text:\")\n",
    "    print(sample_text)\n",
    "    \n",
    "    with open(log_txt_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"\\nTrial {trial_counter} Sample Text:\\n\")\n",
    "        f.write(sample_text)\n",
    "        f.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    log_experiment(trial_counter, trial_config, final_train_loss, final_val_loss, csv_path=log_csv_path)\n",
    "    \n",
    "    trial_counter += 1\n",
    "\n",
    "print(\"\\nGrid search complete. Experiment logs saved to 'lvl2b_log.csv' and sample outputs appended to 'lvl2b_output.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaf069a-22fb-4046-8a10-da4345e39369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
